ğŸš€ LLM Observability Platform

A complete monitoring & analytics system for Large Language Models
built with FastAPI â€¢ Streamlit â€¢ PostgreSQL â€¢ Docker

ğŸŒŸ Overview

The LLM Observability Platform provides real-time insights into the behavior and performance of LLMs.
It helps developers track prompts, responses, tokens, latency, errors, user sessions, model comparisons, and more â€” all through a beautiful and interactive dashboard.

This platform is perfect for developers who want visibility, debugging capabilities, and performance analytics for their LLM applications.

âœ¨ Features
ğŸ”¹ Real-Time LLM Monitoring

Track tokens in/out

Monitor latency per request

Capture model names, statuses & timestamps

Store every request in PostgreSQL

ğŸ”¹ Interactive Playground

Enter prompts and test responses live

Choose models, temperature, and token limits

View latency & token usage instantly

ğŸ”¹ Side-by-Side Model Comparison

Benchmark two LLMs with:

Response differences

Latency comparison

Token usage comparison

ğŸ”¹ Logs Dashboard

Complete history of all requests

Prompt & response preview

Status (success/error)

Sorting & filtering

ğŸ”¹ Analytics & Visual Insights

ğŸ“ˆ Token Usage Trend

âš¡ Latency Trend

âŒ Error Trend

ğŸ“Š Metrics Summary

ğŸ”” Live Alerts

ğŸ”¹ User Session Analytics

Find per-user insights:

Total requests

Total tokens consumed

Average latency

Most-used model

ğŸ”¹ Feedback System

Users can rate responses and leave comments â€” all stored for analysis.

ğŸ³ Docker Support

This system is fully containerized using Docker.

Run everything with:

docker-compose up --build


Docker ensures:

Portability

Easy deployment

Clean isolation

Zero setup issues

Containers include:

FastAPI Backend

Streamlit Frontend

PostgreSQL Database

ğŸ—ï¸ Tech Stack
Layer	Technology
Backend	FastAPI, LangChain, Groq API
Frontend	Streamlit
Database	PostgreSQL
Containerization	Docker
ORM	SQLAlchemy
Validation	Pydantic


ğŸš€ How to Run the Project
1. Clone the Repository
git clone https://github.com/IshaVishwakarma/llm-observability.git
cd llm-observability

2. Add Your Environment Variables

Create a .env file:

GROQ_API_KEY=your_api_key_here


âš ï¸ Your .env file is NOT committed (thanks to .gitignore).

3. Run with Docker
docker-compose up --build

ğŸ“ Default Ports

Backend â†’ http://localhost:9000

Frontend â†’ http://localhost:8501

ğŸ“ Project Structure
llm-observability/
â”‚â”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ crud.py
â”‚   â”œâ”€â”€ callbacks.py
â”‚   â”œâ”€â”€ database.py
â”‚â”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py
â”‚â”€â”€ postgres_data/
â”‚â”€â”€ docker-compose.yml
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md

ğŸ™‹â€â™€ï¸ Author

Isha Vishwakarma
Built with â¤ï¸ using Python, FastAPI, Streamlit, Docker & PostgreSQL.
